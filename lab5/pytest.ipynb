{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Создадим файл model.py, где создаются датасеты и обучается модель"
      ],
      "metadata": {
        "id": "o5JxgrTS3t-M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUPvQfFO3jk1",
        "outputId": "c06f11f5-e82d-466b-91dc-36791166cdde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile model.py\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "def create_dataset():\n",
        "    xs = np.linspace(0, 10, 100)\n",
        "    ys = xs + np.random.random(100) * 2 - 1\n",
        "    return xs, ys\n",
        "\n",
        "dataset1 = create_dataset()\n",
        "dataset2 = create_dataset()\n",
        "dataset3 = create_dataset()\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(dataset1[0].reshape(-1, 1), dataset1[1])\n",
        "\n",
        "def create_noisy_dataset():\n",
        "    xs = np.linspace(0, 10, 100)\n",
        "    ys = xs + np.random.random(100) * 2 - 1\n",
        "    ys[25:45] *= 2\n",
        "    return xs, ys\n",
        "\n",
        "noisy_dataset = create_noisy_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим файл test_model.py, с тестами"
      ],
      "metadata": {
        "id": "st0CU10C3pwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_model.py\n",
        "import pytest\n",
        "from model import model, dataset1, dataset2, dataset3, noisy_dataset\n",
        "\n",
        "def test_model_quality(dataset):\n",
        "    prediction = model.predict(dataset[0].reshape(-1, 1))\n",
        "    mse = ((prediction - dataset[1]) ** 2).mean()\n",
        "    assert mse < 0.5, \"Model's quality is unsatisfactory\"\n",
        "\n",
        "def test_datasets():\n",
        "    test_model_quality(dataset1)\n",
        "    test_model_quality(dataset2)\n",
        "    test_model_quality(dataset3)\n",
        "    test_model_quality(noisy_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ri2bvYv3kc1",
        "outputId": "f1507999-cfa4-44dc-f68e-1d5ef66fa356"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запустим pytest"
      ],
      "metadata": {
        "id": "tZ_BmL5T31Fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -v test_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CztZiveT3l__",
        "outputId": "b7896fd1-3989-4704-fa1a-bf2c3018a749"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.2.2, pluggy-1.0.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: anyio-3.6.2\n",
            "collected 2 items                                                              \u001b[0m\n",
            "\n",
            "test_model.py::test_model_quality \u001b[31mERROR\u001b[0m\u001b[31m                                  [ 50%]\u001b[0m\n",
            "test_model.py::test_datasets \u001b[31mFAILED\u001b[0m\u001b[31m                                      [100%]\u001b[0m\n",
            "\n",
            "==================================== ERRORS ====================================\n",
            "\u001b[31m\u001b[1m_____________________ ERROR at setup of test_model_quality _____________________\u001b[0m\n",
            "file /content/test_model.py, line 4\n",
            "  def test_model_quality(dataset):\n",
            "\u001b[31mE       fixture 'dataset' not found\u001b[0m\n",
            "\u001b[31m>       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory\u001b[0m\n",
            "\u001b[31m>       use 'pytest --fixtures [testpath]' for help on them.\u001b[0m\n",
            "\n",
            "/content/test_model.py:4\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m________________________________ test_datasets _________________________________\u001b[0m\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_datasets\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "        test_model_quality(dataset1)\u001b[90m\u001b[39;49;00m\n",
            "        test_model_quality(dataset2)\u001b[90m\u001b[39;49;00m\n",
            "        test_model_quality(dataset3)\u001b[90m\u001b[39;49;00m\n",
            ">       test_model_quality(noisy_dataset)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_model.py\u001b[0m:13: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "dataset = (array([ 0.        ,  0.1010101 ,  0.2020202 ,  0.3030303 ,  0.4040404 ,\n",
            "        0.50505051,  0.60606061,  0.70707071,...22299,  9.61119181,  9.84087262,  9.20120043,\n",
            "        8.78526042, 10.51189538,  9.30355981, 10.64504453, 10.69685274]))\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_model_quality\u001b[39;49;00m(dataset):\u001b[90m\u001b[39;49;00m\n",
            "        prediction = model.predict(dataset[\u001b[94m0\u001b[39;49;00m].reshape(-\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
            "        mse = ((prediction - dataset[\u001b[94m1\u001b[39;49;00m]) ** \u001b[94m2\u001b[39;49;00m).mean()\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m mse < \u001b[94m0.5\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mModel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms quality is unsatisfactory\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: Model's quality is unsatisfactory\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert 2.877889489150018 < 0.5\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_model.py\u001b[0m:7: AssertionError\n",
            "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m test_model.py::\u001b[1mtest_datasets\u001b[0m - AssertionError: Model's quality is unsatisfactory\n",
            "\u001b[31mERROR\u001b[0m test_model.py::\u001b[1mtest_model_quality\u001b[0m\n",
            "\u001b[31m========================== \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[31m\u001b[1m1 error\u001b[0m\u001b[31m in 2.34s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NJXyzGNj3r6p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}